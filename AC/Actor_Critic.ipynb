{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oq2uBLgkNY09"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import sys\n",
    "from jax import tree_map\n",
    "from jax import grad, vjp, value_and_grad\n",
    "from jax import lax\n",
    "from jax import jit\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjX9aTaFQa0x"
   },
   "outputs": [],
   "source": [
    "def policy_fn(observation):\n",
    "  mlp = hk.Sequential(\n",
    "      [\n",
    "       hk.Linear(16), nn.relu, \n",
    "       hk.Linear(2)\n",
    "      ]\n",
    "  )\n",
    "  return mlp(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnlj4GQQYZmG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def val_fn(observation):\n",
    "  mlp = hk.Sequential(\n",
    "      [\n",
    "       hk.Linear(32), nn.relu, \n",
    "       hk.Linear(1)\n",
    "      ]\n",
    "  )\n",
    "  return mlp(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOYKj9PgHC2c"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "  # Define training configurations and problem parameters\n",
    "  NUM_EPISODE = 400\n",
    "  GAMMA = 0.99\n",
    "  RECORD_INTERVAL = 2\n",
    "  LAMBDA_ACTOR = 1.0\n",
    "  LAMBDA_CRITIC = 0.7\n",
    "  \n",
    "  # random key generator\n",
    "  rng = hk.PRNGSequence(0)\n",
    "  \n",
    "  # actor and critic networks\n",
    "  actor = hk.without_apply_rng(hk.transform(policy_fn))\n",
    "  critic = hk.without_apply_rng(hk.transform(val_fn))\n",
    "\n",
    "  dummy_obs = jnp.array([0,0,0,0],dtype=jnp.float32)\n",
    "\n",
    "  actor_params = actor.init(rng.next(), dummy_obs)\n",
    "  critic_params = critic.init(rng.next(), dummy_obs)\n",
    "\n",
    "  # make optimizers\n",
    "  actor_opt = optax.adam(1e-4)\n",
    "  critic_opt = optax.adam(1e-4)\n",
    "\n",
    "  actor_opt_state = actor_opt.init(actor_params)\n",
    "  critic_opt_state = critic_opt.init(critic_params)\n",
    "\n",
    "  # actor-related functions\n",
    "  @jit\n",
    "  def actor_obj(params, obs, rand):\n",
    "    logits = actor.apply(params, obs)\n",
    "    action = lax.stop_gradient(random.categorical(rand,logits))\n",
    "    log_likelihood = nn.log_softmax(logits)\n",
    "    return -log_likelihood[action], action \n",
    "  \n",
    "  @jit\n",
    "  def update_actor(params, opt_state, tde, trace):\n",
    "    increment = tree_map(lambda t: t*tde, trace)\n",
    "    updates, new_opt_state = actor_opt.update(increment, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "  \n",
    "  @jit\n",
    "  def accumulate_actor_grad(cum_grad, log_grad, discount):\n",
    "    new_cum_grad = tree_map(lambda cg, lg: GAMMA*LAMBDA_ACTOR*cg + discount*lg, cum_grad, log_grad)\n",
    "    return new_cum_grad\n",
    "  \n",
    "  \n",
    "  @jit\n",
    "  def zero_tree(tree):\n",
    "    return tree_map(lambda x: 0*x, tree)\n",
    "  \n",
    "  \n",
    "  # critic-related functions\n",
    "  @jit\n",
    "  def critic_obj(params, obs, reward, next_obs, gamma):\n",
    "    v_t = critic.apply(params, obs)\n",
    "    td_error = lax.stop_gradient(reward + gamma*critic.apply(params, next_obs) - v_t)\n",
    "    return -v_t[0], td_error\n",
    "  \n",
    "  @jit\n",
    "  def update_critic(params, opt_state, tde, trace):\n",
    "    increment = tree_map(lambda t: t*tde, trace)\n",
    "    updates, new_opt_state = critic_opt.update(increment, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "  \n",
    "  @jit\n",
    "  def accumulate_critic_grad(cum_grad, grad):\n",
    "    new_cum_grad = tree_map(lambda cg, g: GAMMA*LAMBDA_CRITIC*cg + g, cum_grad, grad)\n",
    "    return new_cum_grad\n",
    "\n",
    "  \n",
    "  # environment\n",
    "  env = gym.make('CartPole-v0')\n",
    "  env.seed(1)\n",
    "  \n",
    "  # record containers\n",
    "  episode_lengths = []\n",
    "  avg_tde = []\n",
    "\n",
    "  for eps in tqdm(range(NUM_EPISODE)):\n",
    "    # initialization for each episode\n",
    "    o_t = jnp.array(env.reset(), dtype=jnp.float32)\n",
    "    done = False\n",
    "    I = 1.0\n",
    "    cumulative_tde = 0\n",
    "    num_step = 0\n",
    "    actor_trace = zero_tree(actor_params)\n",
    "    critic_trace = zero_tree(critic_params)\n",
    "    while not done:\n",
    "      likelihood_grad, action = grad(actor_obj, has_aux=True)(actor_params, o_t, rng.next())\n",
    "      \n",
    "      # act in environment\n",
    "      o_tp1, reward, done, _ = env.step(action.item())\n",
    "      o_tp1 = jnp.array(o_tp1, dtype=jnp.float32)\n",
    "      num_step += 1\n",
    "      \n",
    "      # update critic\n",
    "      critic_grad, tde = grad(critic_obj,has_aux=True)(critic_params, o_t, reward, o_tp1, GAMMA)\n",
    "      cumulative_tde += abs(tde.item())\n",
    "      critic_trace = accumulate_critic_grad(critic_trace, critic_grad)\n",
    "      critic_params, critic_opt_state = update_critic(critic_params, critic_opt_state, tde, critic_trace)\n",
    "      # update actor\n",
    "      actor_trace = accumulate_actor_grad(actor_trace, likelihood_grad, I)\n",
    "      actor_params, actor_opt_state = update_actor(actor_params, actor_opt_state, tde, actor_trace)\n",
    "      \n",
    "      I = GAMMA*I\n",
    "      o_t = o_tp1\n",
    "    \n",
    "    # recording\n",
    "    if eps % RECORD_INTERVAL == 0:\n",
    "      episode_lengths.append(num_step)\n",
    "      avg_tde.append(cumulative_tde/num_step)\n",
    "    \n",
    "  # save models\n",
    "  with open('actor_param.pickle','wb') as fp:\n",
    "    pkl.dump(actor_params, fp)\n",
    "\n",
    "  with open('critic_param.pickle','wb') as fp:\n",
    "    pkl.dump(critic_params, fp)\n",
    "  \n",
    "  return episode_lengths, avg_tde\n",
    " \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(episode_lengths, avg_tde):\n",
    "  plt.figure()\n",
    "  plt.plot(episode_lengths,'b')\n",
    "  plt.xlabel('episode/interval')\n",
    "  plt.ylabel('length')\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(avg_tde,'r')\n",
    "  plt.xlabel('episode/interval')\n",
    "  plt.ylabel('TD-error')\n",
    "  plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episodes, render=False):\n",
    "  GAMMA = 0.95\n",
    "  # load parameters\n",
    "  with open('actor_param.pickle','rb') as fp:\n",
    "    actor_params = pkl.load(fp)\n",
    "  with open('critic_param.pickle','rb') as fp:\n",
    "    critic_params = pkl.load(fp)\n",
    "  \n",
    "  # create actor and critic\n",
    "  actor = hk.without_apply_rng(hk.transform(policy_fn))\n",
    "  critic = hk.without_apply_rng(hk.transform(val_fn))\n",
    "  \n",
    "  rng = hk.PRNGSequence(0)\n",
    "  \n",
    "  def act(obs, rng):\n",
    "    logits = actor.apply(actor_params, obs)\n",
    "    return random.categorical(rng, logits=logits).item()\n",
    "  \n",
    "  def pred(obs):\n",
    "    return critic.apply(critic_params, obs).item()\n",
    "  # make environment\n",
    "  env = gym.make('CartPole-v0')\n",
    "  env.seed(0)\n",
    "  \n",
    "  eps_lengths = []\n",
    "  avg_tdes = []\n",
    "  for eps in range(episodes):\n",
    "    o_t = jnp.array(env.reset())\n",
    "    done = False\n",
    "    num_steps = 0\n",
    "    total_tde = 0\n",
    "    while not done:\n",
    "      action = act(o_t, rng.next())\n",
    "      \n",
    "      o_tp1, reward, done, _ = env.step(action)\n",
    "      num_steps += 1\n",
    "      \n",
    "      if render:\n",
    "        env.render()\n",
    "      \n",
    "      total_tde += abs(reward + GAMMA*pred(o_tp1) - pred(o_t))\n",
    "      \n",
    "      o_t = o_tp1\n",
    "      \n",
    "    \n",
    "    eps_lengths.append(num_steps)\n",
    "    avg_tdes.append(total_tde/num_steps)\n",
    "  \n",
    "  env.close()\n",
    "  return eps_lengths, avg_tdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  eps_len, avg_tde = train()\n",
    "  plot_figure(eps_len, avg_tde)\n",
    "  eps_len, avg_tde = test(100)\n",
    "  plot_figure(eps_len, avg_tde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Actor-Critic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
